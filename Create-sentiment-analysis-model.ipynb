{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備\n",
    "依存関係をインストールします"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -U pip\n",
    "!pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install transformers[\"ja\"] numpy pandas sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習データのダウンロード\n",
    "今回は株式会社リクルートが提供する\"Japanese Realistic Textual Entailment Corpus\" (``https://github.com/megagonlabs/jrte-corpus``)を学習データとして利用します  \n",
    "**このデータセットライセンスは `CC BY-NC-SA 4.0` なので営利目的には利用できません。**\n",
    "### 参考\n",
    "林部祐太．\n",
    "知識の整理のための根拠付き自然文間含意関係コーパスの構築．\n",
    "言語処理学会第26回年次大会論文集，pp.820-823. 2020. (NLP 2020)\n",
    "[[PDF]](https://www.anlp.jp/proceedings/annual_meeting/2020/pdf_dir/P4-9.pdf)\n",
    "[[Poster]](https://storage.googleapis.com/megagon-publications/nlp2020/p4-9_hayashibe_poster.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/megagonlabs/jrte-corpus/master/data/pn.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ダウンロードしたデータの中身を確認してみます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head pn.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ヘッダがないので付与しつつpandasのdata frameとして読み込みます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"pn.tsv\", names=(\"id\", \"label\", \"text\", \"judges\", \"usage\"), sep=\"\\t\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習データの分割\n",
    "学習データを前処理しつつ学習用/検証用/テスト用に分割します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace({\"label\": {0: 2, 1: 1, -1: 0}})\n",
    "\n",
    "train_data = []\n",
    "eval_data = []\n",
    "test_data = []\n",
    "for i, row in df.iterrows():\n",
    "    if row[\"usage\"] == \"train\":\n",
    "        train_data.append({\"x\": row[\"text\"], \"y\": row[\"label\"]})\n",
    "    elif row[\"usage\"] == \"dev\":\n",
    "        eval_data.append({\"x\": row[\"text\"], \"y\": row[\"label\"]})\n",
    "    elif row[\"usage\"] == \"test\":\n",
    "        test_data.append({\"x\": row[\"text\"], \"y\": row[\"label\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの準備\n",
    "次に、学習に利用するモデルを用意していきます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AlbertTokenizerFast, AlbertForSequenceClassification\n",
    "\n",
    "label2id = {\"Negative\": 0, \"Positive\": 1, \"Neutral\": 2}\n",
    "base_model = \"ken11/albert-base-japanese-v1\"\n",
    "tokenizer = AlbertTokenizerFast.from_pretrained(base_model)\n",
    "model = AlbertForSequenceClassification.from_pretrained(base_model, label2id=label2id, id2label={0: \"Negative\", 1: \"Positive\", 2: \"Neutral\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainerの準備\n",
    "TrainingArgumentsを設定し、Trainerを作成していきます  \n",
    "Trainerにはdata_collatorを渡してあげる必要があるので、data_collatorも作成します  \n",
    "  \n",
    "data_collatorはtransformersにすでにあるものを利用することもできますが、ここでは自前で定義していきます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def data_collator(features: list) -> dict:\n",
    "    x = [f[\"x\"] for f in features]\n",
    "    y = [f[\"y\"] for f in features]\n",
    "    inputs = tokenizer(x, return_tensors=None, padding='max_length', truncation=True, max_length=128)\n",
    "    input_labels = []\n",
    "    for label in y:\n",
    "        input_labels.append([label])\n",
    "    inputs['labels'] = input_labels\n",
    "    batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in inputs.items()}\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パラメータ類を設定しておきます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = \"./ckpt\"\n",
    "batch_size = 16\n",
    "epochs = 3\n",
    "learning_rate = 3e-5\n",
    "save_freq = 100\n",
    "model_output_dir = \"./dest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(output_dir=ckpt_dir,\n",
    "                         do_train=True,\n",
    "                         do_eval=True,\n",
    "                         do_predict=True,\n",
    "                         per_device_train_batch_size=batch_size,\n",
    "                         per_device_eval_batch_size=batch_size,\n",
    "                         learning_rate=learning_rate,\n",
    "                         num_train_epochs=epochs,\n",
    "                         evaluation_strategy=\"steps\",\n",
    "                         eval_steps=save_freq,\n",
    "                         save_strategy=\"steps\",\n",
    "                         save_steps=save_freq,\n",
    "                         load_best_model_at_end=True,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, EarlyStoppingCallback\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  args=args,\n",
    "                  data_collator=data_collator,\n",
    "                  train_dataset=train_data,\n",
    "                  eval_dataset=eval_data,\n",
    "                  callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習を実行します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "できあがったモデルをテストします"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, metrics = trainer.predict(test_data, metric_key_prefix=\"test\")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルを保存します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トークナイザーは特に変更をしていませんが、使うときのために一緒に保存しておきましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(model_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論\n",
    "できあがったモデルを使って推論を行ってみます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def inference(text: str):\n",
    "    model = AlbertForSequenceClassification.from_pretrained(model_output_dir)\n",
    "    tokenizer = AlbertTokenizerFast.from_pretrained(model_output_dir)\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=128)\n",
    "    pred = model(**inputs).logits[0]\n",
    "    pred = np.argmax(pred.detach().numpy(), axis=-1)\n",
    "    print(f\"input text: {text}\\nsentiment: {model.config.id2label[pred]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(\"駅から近くて便利でした\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
